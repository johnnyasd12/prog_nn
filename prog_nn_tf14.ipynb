{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-03T06:25:44.161673Z",
     "start_time": "2018-11-03T06:25:43.206023Z"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "from param_collection import ParamCollection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-03T06:26:14.607606Z",
     "start_time": "2018-11-03T06:26:14.605317Z"
    }
   },
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def weight_variable(shape, stddev=0.1, initial=None):\n",
    "    if initial is None:\n",
    "        initial = tf.truncated_normal(shape, stddev=stddev, dtype=tf.float64)\n",
    "    return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-03T06:26:22.883842Z",
     "start_time": "2018-11-03T06:26:22.876620Z"
    }
   },
   "outputs": [],
   "source": [
    "def bias_variable(shape, init_bias=0.1, initial=None):\n",
    "    if initial is None:\n",
    "        initial = tf.constant(init_bias, shape=shape, dtype=tf.float64)\n",
    "    return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-03T06:26:31.936193Z",
     "start_time": "2018-11-03T06:26:31.930683Z"
    }
   },
   "outputs": [],
   "source": [
    "class InitialColumnProgNN(object):\n",
    "    \"\"\"\n",
    "    Descr: Initial network to train for later use transfer learning with a\n",
    "        Progressive Neural Network.\n",
    "    Args:\n",
    "        topology - A list of number of units in each hidden dimension.\n",
    "                   First entry is input dimension.\n",
    "        activations - A list of activation functions to use on the transforms.\n",
    "        session - A TensorFlow session.\n",
    "    Returns:\n",
    "        None - attaches objects to class for InitialColumnProgNN.session.run()\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, topology, activations, session, dtype=tf.float64):\n",
    "        n_input = topology[0]\n",
    "        # Layers in network.\n",
    "        L = len(topology) - 1 # n_hidden_layer?\n",
    "        self.session = session\n",
    "        self.L = L\n",
    "        self.topology = topology\n",
    "        self.o_n = tf.placeholder(dtype,shape=[None, n_input])\n",
    "\n",
    "        self.W = []\n",
    "        self.b =[]\n",
    "        self.h = [self.o_n]\n",
    "        params = []\n",
    "        for k in range(L):\n",
    "            shape = topology[k:k+2]\n",
    "            self.W.append(weight_variable(shape))\n",
    "            self.b.append(bias_variable([shape[1]]))\n",
    "            self.h.append(activations[k](tf.matmul(self.h[-1], self.W[k]) + self.b[k]))\n",
    "            params.append(self.W[-1])\n",
    "            params.append(self.b[-1])\n",
    "        self.pc = ParamCollection(self.session, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-03T06:26:41.610710Z",
     "start_time": "2018-11-03T06:26:41.583289Z"
    }
   },
   "outputs": [],
   "source": [
    "class ExtensibleColumnProgNN(object):\n",
    "    \"\"\"\n",
    "    Descr: An extensible network column for use in transfer learning with a\n",
    "        Progressive Neural Network.\n",
    "    Args:\n",
    "        topology - A list of number of units in each hidden dimension.\n",
    "            First entry is input dimension.\n",
    "        activations - A list of activation functions to use on the transforms.\n",
    "        session - A TensorFlow session.\n",
    "        prev_columns - Previously trained columns, either Initial or Extensible,\n",
    "            we are going to create lateral connections to for the current column.\n",
    "    Returns:\n",
    "        None - attaches objects to class for ExtensibleColumnProgNN.session.run()\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, topology, activations, session, prev_columns, dtype=tf.float64):\n",
    "        n_input = topology[0]\n",
    "        self.topology = topology\n",
    "        self.session = session\n",
    "        width = len(prev_columns)\n",
    "        # Layers in network. First value is n_input, so it doesn't count.\n",
    "        L = len(topology) -1\n",
    "        self.L = L\n",
    "        self.prev_columns = prev_columns\n",
    "\n",
    "        # Doesn't work if the columns aren't the same height.\n",
    "        assert all([self.L == x.L for x in prev_columns])\n",
    "\n",
    "        self.o_n = tf.placeholder(dtype, shape=[None, n_input])\n",
    "\n",
    "        self.W = [[]] * L\n",
    "        self.b = [[]] * L\n",
    "        self.U = []\n",
    "        for k in range(L-1):\n",
    "            self.U.append( [[]] * width )\n",
    "        self.h = [self.o_n]\n",
    "        # Collect parameters to hand off to ParamCollection.\n",
    "        params = []\n",
    "        for k in range(L):\n",
    "            W_shape = topology[k:k+2]\n",
    "            self.W[k] = weight_variable(W_shape)\n",
    "            self.b[k] = bias_variable([W_shape[1]])\n",
    "            if k == 0:\n",
    "                self.h.append(activations[k](tf.matmul(self.h[-1],self.W[k]) + self.b[k]))\n",
    "                params.append(self.W[k])\n",
    "                params.append(self.b[k])\n",
    "                continue\n",
    "            preactivation = tf.matmul(self.h[-1],self.W[k]) + self.b[k]\n",
    "            for kk in range(width):\n",
    "                U_shape = [prev_columns[kk].topology[k], topology[k+1]]\n",
    "                # Remember len(self.U) == L - 1!\n",
    "                self.U[k-1][kk] = weight_variable(U_shape)\n",
    "                # pprint(prev_columns[kk].h[k].get_shape().as_list())\n",
    "                # pprint(self.U[k-1][kk].get_shape().as_list())\n",
    "                preactivation +=  tf.matmul(prev_columns[kk].h[k],self.U[k-1][kk])\n",
    "            self.h.append(activations[k](preactivation))\n",
    "            params.append(self.W[k])\n",
    "            params.append(self.b[k])\n",
    "            for kk in range(width):\n",
    "                params.append(self.U[k-1][kk])\n",
    "\n",
    "        self.pc = ParamCollection(self.session, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-03T06:27:15.274245Z",
     "start_time": "2018-11-03T06:27:14.729442Z"
    }
   },
   "outputs": [],
   "source": [
    "# main\n",
    "\n",
    "# Make some fake observations.\n",
    "fake1 = np.float64(np.random.rand(4000,128))\n",
    "fake2 = np.float64(np.random.rand(4000,128))\n",
    "fake3 = np.float64(np.random.rand(4000,128))\n",
    "fake4 = np.float64(np.random.rand(4000,128))\n",
    "fake5 = np.float64(np.random.rand(4000,128))\n",
    "n_input = 128\n",
    "topology1 = [n_input, 100, 64, 25, 9]\n",
    "topology2 = [n_input, 68, 44, 19, 7]\n",
    "topology3 = [n_input, 79, 58, 33, 12]\n",
    "topology4 = [n_input, 40, 30, 20, 10]\n",
    "topology5 = [n_input, 101, 73, 51, 8]\n",
    "activations = [tf.nn.relu, tf.nn.relu, tf.nn.relu, tf.nn.softmax]\n",
    "\n",
    "session = tf.Session()\n",
    "session.run(tf.global_variables_initializer())\n",
    "\n",
    "col_0 = InitialColumnProgNN(topology1, activations, session)\n",
    "th0 = col_0.pc.get_values_flat()\n",
    "col_1 = ExtensibleColumnProgNN(topology2, activations, session, [col_0])\n",
    "th1 = col_1.pc.get_values_flat()\n",
    "col_2 = ExtensibleColumnProgNN(topology3, activations, session, [col_0, col_1])\n",
    "th2 = col_2.pc.get_values_flat()\n",
    "col_3 = ExtensibleColumnProgNN(topology4, activations, session, [col_0, col_1, col_2])\n",
    "th3 = col_3.pc.get_values_flat()\n",
    "col_4 = ExtensibleColumnProgNN(topology5, activations, session, [col_0, col_1, col_2, col_3])\n",
    "th4 = col_4.pc.get_values_flat()\n",
    "\n",
    "# This pattern to evaluate the Progressive NN can be extended to a\n",
    "# arbitrarily large number of columns / models.\n",
    "\n",
    "# Fake train the first network. h_0[-1] has information loss functions need.\n",
    "h_0 = col_0.session.run([col_0.h],\n",
    "    feed_dict={col_0.o_n:fake1})\n",
    "\n",
    "# Fake train the second network, but this time with lateral connections to\n",
    "# fake pre-trained, constant weights from first column of Progressive NN.\n",
    "h_1 = col_1.session.run([col_1.h],\n",
    "    feed_dict={col_1.o_n:fake2, col_1.prev_columns[0].o_n:fake2})\n",
    "\n",
    "# Now fake train a third column that has lateral connections to both\n",
    "# previously \"trained\" columns.\n",
    "h_2 = col_2.session.run([col_2.h],\n",
    "    feed_dict={col_2.o_n:fake3,\n",
    "        col_2.prev_columns[0].o_n:fake3,\n",
    "        col_2.prev_columns[1].o_n:fake3})\n",
    "\n",
    "# Fourth column / fake instance of training.\n",
    "h_3 = col_3.session.run([col_3.h],\n",
    "    feed_dict={col_3.o_n:fake4,\n",
    "        col_3.prev_columns[0].o_n:fake4,\n",
    "        col_3.prev_columns[1].o_n:fake4,\n",
    "        col_3.prev_columns[2].o_n:fake4})\n",
    "\n",
    "# Fifth column. Notice we have to pass in n placeholder with the same\n",
    "# obsevations to a Progressive NN with n columns.\n",
    "h_4 = col_4.session.run([col_4.h],\n",
    "    feed_dict={col_4.o_n:fake5,\n",
    "        col_4.prev_columns[0].o_n:fake5,\n",
    "        col_4.prev_columns[1].o_n:fake5,\n",
    "        col_4.prev_columns[2].o_n:fake5,\n",
    "        col_4.prev_columns[3].o_n:fake5})\n",
    "\n",
    "# Anyway, you get the drift. Hope this helps someone understand\n",
    "# Progressive Neural Networks!\n",
    "\n",
    "# Make sure the column parameters aren't changing when being used by\n",
    "# later columns.\n",
    "\n",
    "# Should be a list of [0., 0., 0., ... 0.] if theta isn't changing.\n",
    "# We add 1.0 to each element to see if they were all zero with np.all().\n",
    "assert np.all(col_4.prev_columns[0].pc.get_values_flat() - th0 + 1.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tu36]",
   "language": "python",
   "name": "conda-env-tu36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
